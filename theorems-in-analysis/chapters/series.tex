\setchapterpreamble{\dictum[Niels Henrik Abel, in a letter to Holmboe
  (1826)]{``Divergent series are the invention of the devil, and it is
a shame to base on them any demonstration whatsoever.''}}
\chapter{Series}
\begin{definition}[Infinite series]
  \phantomsection
  \addcontentsline{toc}{section}{Infinite series}
  \deflabel{infinite-series}
  A sum of infinitely many numbers is called an \vocab{infinite
  series} and is denoted by
  \[ \sum_{k = 1}^{\infty} a_k \qquad {\text{or}} \qquad \lim_{n \to
  \infty} \sum_{k = 1}^{n} a_k. \]
\end{definition}

\begin{definition}[Terms]
  Given an infinite series $\sum_{k = 1}^{\infty} a_k$, the numbers
  $a_k$ are the \vocab{terms} of the series.
\end{definition}

\begin{definition}[Sequence of partial sums]
  \phantomsection
  \addcontentsline{toc}{section}{Sequence of partial sums}
  Given an infinite series $\sum_{k = 1}^{\infty} a_k$, the
  \vocab{sequence of partial sums} is the sequence
  \[ \left(\sum_{k = 1}^{n} a_k\right)_{n = 1}^{\infty}. \]
  That is, it's the sequence $(s_n)$ where:
  \begin{align*}
    s_1 & = a_1 \\
    s_2 & = a_1 + a_2 \\
    s_3 & = a_1 + a_2 + a_3 \\
    s_4 & = a_1 + a_2 + a_3 + a_4 \\
    & \,\,\, \vdots
  \end{align*}
\end{definition}

\begin{definition}[Convergent and divergent series]
  \phantomsection
  \addcontentsline{toc}{section}{Convergent and divergent series}
  \deflabel{convergent-divergent-series}
  Given an infinite series $\sum_{k = 1}^{\infty} a_k$, the series
  \vocab{converges} to $L \in \RR$, denoted by
  \[ \sum_{k = 1}^{\infty} a_k = L, \]
  if the sequence of partial sums $(s_n)$ converges to $L$.

  The series \vocab{diverges} (to $\infty$, to $-\infty$, or does not
  exist) if $(s_n)$ does.
\end{definition}

\begin{definition}[Bounded series]
  \phantomsection
  \addcontentsline{toc}{section}{Bounded series}
  Given an infinite series $\sum_{k = 1}^{\infty} a_k$, the series
  is \vocab{bounded} if the sequence of partial sums $(s_n)$ is bounded.
\end{definition}

\begin{definition}[Monotone series]
  \phantomsection
  \addcontentsline{toc}{section}{Monotone series}
  Given an infinite series $\sum_{k = 1}^{\infty} a_k$, the series is
  \vocab{monotone} if the sequence of partial sums $(s_n)$ is monotone.
\end{definition}

\begin{theorem}[Series limit laws]
  \phantomsection
  \addcontentsline{toc}{section}{Series limit laws}
  \thmlabel{series-limit-laws}
  Assume that $\sum_{k = 1}^{\infty} a_k = \alpha$ and $\sum_{k =
  1}^{\infty} b_k = \beta$.
  Also assume that $c \in \RR$. Then,
  \begin{enumerate}
    \item $\sum_{k = 1}^{\infty} (a_k + b_k) = \alpha + \beta$,
    \item $\sum_{k = 1}^{\infty} (a_k - b_k) = \alpha - \beta$, and
    \item $\sum_{k = 1}^{\infty} (c \cdot a_k) = c \cdot \alpha$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  We proceed with a proof of part 1. Let
  \[ s_n = \sum_{k = 1}^{n} a_k \qquad \text{and} \qquad t_n =
  \sum_{k = 1}^{n} b_k. \]
  Now, $(s_n)$ is the sequence of partial sums of the series $\sum_{k
  = 1}^{\infty} a_k$ and $(t_n)$ is the sequence of partial sums of
  the series $\sum_{k = 1}^{\infty} b_k$. Therefore, by assumption
  and \defref{convergent-divergent-series}, we have that
  \[ s_n \to \alpha \qquad \text{and} \qquad t_n \to \beta. \]
  Now let $v_n = \sum_{k = 1}^{n} (a_k + b_k)$, and since we can
  rearrange finite sums,
  \begin{align*}
    s_n + t_n & = (a_1 + a_2 + \dots + a_n) + (b_1 + b_2 + \dots b_n) \\
    & = (a_1 + b_1) + (a_2 + b_2) + \dots + (a_n + b_n) \\
    & = v_n.
  \end{align*}
  Collecting everything,
  \begin{align*}
    \sum_{k = 1}^{\infty} (a_k + b_k) & = \lim_{n \to \infty} \sum_{k = 1}^{n}
    (a_k + b_k) && \text{(by \defref{infinite-series})} \\
    & = \lim_{n \to \infty} (v_n) && \text{(by definition of $v_n$)} \\
    & = \lim_{n \to \infty} (s_n + t_n) && \text{(showed above)} \\
    & = \alpha + \beta && \text{(by \thmref{sequence-limit-laws})}
  \end{align*}
  as desired.

  In almost exactly the same way we can prove part 2 and part 3.
\end{proof}

\begin{remark}
  Define $\sum_{k = 1}^{\infty} a_k$ and $\sum_{k = 1}^{\infty} b_k$
  as in \thmref{series-limit-laws}. We cannot conclude that $\sum_{k
  = 1}^{\infty} (a_k \cdot b_k) = \alpha \cdot \beta$ since, in general,
  \[ \left(\sum_{k = 1}^{n} a_k\right) \left(\sum_{k = 1}^{n}
  b_k\right) \neq \sum_{k = 1}^{n} (a_k \cdot b_k). \]
\end{remark}

\begin{theorem}[Cauchy criterion for series]
  \phantomsection
  \addcontentsline{toc}{section}{Cauchy criterion for series}
  \thmlabel{cauchy-criterion-for-series}
  The series $\sum_{k = 1}^{\infty} a_k$ converges if and only if for
  every $\epsilon > 0$, there exists an $N \in \NN$ such that
  \[ \left|\sum_{k = n}^{m} a_k\right| < \epsilon \qquad
  \text{whenever $m \geq n > N$.}\]
\end{theorem}

\begin{proof}
  Observe that
  \[ \abs{s_m - s_{n - 1}} = \left|\sum_{k = 1}^{m} a_k - \sum_{k =
  1}^{n - 1} a_k\right| = \left|\sum_{k = n}^{m} a_k\right| \]
  and apply the Cauchy criterion for convergence
  (\thmref{cauchy-criterion-for-convergence}) to conclude that
  $(s_n)$ converges, which by \defref{convergent-divergent-series}
  means that $\sum_{k = 1}^{\infty} a_k$ converges.
\end{proof}

\begin{theorem}[Necessary condition for convergence]
  \phantomsection
  \addcontentsline{toc}{section}{Necessary condition for convergence}
  \thmlabel{necessary-condition-for-convergence}
  If $\sum_{k = 1}^{\infty} a_k$ converges, then $a_k \to 0$.
\end{theorem}

\begin{proof}
  Consider the special case $m = n + 1$ in \thmref{cauchy-criterion-for-series}.
\end{proof}

\begin{lemma}
  \lemlabel{series-with-nonnegative-sequence}
  If $a_k \geq 0$ for all $k \in \NN$, then $\sum_{k = 1}^{\infty}
  a_k$ either converges or it diverges to $\infty$.
\end{lemma}

\begin{proof}
  Since each $a_k$ is nonnegative, observe that the sequence of
  partial sums is monotone increasing. Applying the monotone
  convergence theorem (\thmref{monotone-convergence}) then gives the result.
\end{proof}

\begin{proposition}[Comparison test]
  \phantomsection
  \addcontentsline{toc}{section}{Comparison test}
  \prplabel{comparison-test}
  Assume $0 \leq a_k \leq b_k$ for all $k \in \NN$.
  \begin{enumerate}
    \item If $\sum_{k = 1}^{\infty} b_k$ converges, then $\sum_{k =
      1}^{\infty} a_k$ converges.
    \item If $\sum_{k = 1}^{\infty} a_k$ diverges, then $\sum_{k =
      1}^{\infty} b_k$ diverges.
  \end{enumerate}
\end{proposition}

\begin{proof}
  Let $(s_n)$ be the sequence of partial sums of $\sum_{k =
  1}^{\infty} a_k$, and let $(t_n)$ be the sequence of partial sums
  of $\sum_{k = 1}^{\infty} b_k$. And here is an observation that
  will be useful:
  Note that since $a_k \leq b_k$ for all $k$,
  \[ s_n = a_1 + a_2 + \dots + a_n \leq b_1 + b_2 + \dots + b_n = t_n \]
  for all $n$. That is,
  \[ s_n \leq t_n \qquad \text{for all $n$.} \tag{$\bigstar$} \]
  We proceed with a proof of part 1, but first note that by
  \lemref{series-with-nonnegative-sequence} that all we need to show
  is that $(s_n)$ is bounded above. If we can do this we know that
  $s_n \not\to \infty$, which by that proposition implies that it
  must converge.

  Recall that in \prpref{convergence-implies-bounded} we showed that
  if a sequence converges, then it is bounded. And so, since by
  assumption $\sum_{k = 1}^{\infty} b_k$ converges (meaning that
  $(t_n)$ converges), we deduce that this is series is bounded above
  by some value $M$. That is, $t_n \leq M$ for all $n$ (this is
  \defref{bounded-sequences}). And so by $(\bigstar)$, $s_n \leq t_n
  \leq M$ for all $n$, showing that $(s_n)$ is also bounded above by
  $M$, completing the proof of part 1.

  The proof of part 2 is similar. Let $M > 0$. Assume that $\sum_{k =
  1}^{\infty} a_k$ diverges, meaning that $(s_n)$ diverges (and hence
    by \lemref{series-with-nonnegative-sequence} this means it diverges
  to $\infty$). And so by \defref{divergent-sequences}, there exists
  some $N$ for which $M < s_n$ for all $n > N$. And so by
  $(\bigstar)$, $M < s_n \leq t_n$ for all $n > N$. Thus, again
  by \defref{divergent-sequences}, $(t_n)$ also diverges to $\infty$.
\end{proof}

Here's an alternative proof that is much shorter.

\begin{proof}
  Both part 1 and part 2 follows immediately from
  \thmref{cauchy-criterion-for-series} and the observation that
  \[ \left|\sum_{k = n}^{m} a_k\right| \leq \left|\sum_{k = n}^{m}
  b_k\right| \]
  for all $m, n \in \NN$.
\end{proof}

\begin{remark}
  Observe that if, say, $0 \leq a_k \leq b_k$ for all but the first
  10 terms, then the conclusion of the theorem would still hold.
  Based on the Cauchy criterion, pbserve that the behavior of a series is
  determined only by its tails. Thus, in the comparison test, the
  requirement that $a_k \leq b_k$ does not really need to hold for
  all $n \in \NN$ but just needs to be eventually true, i.e. as long
  as $a_k \leq b_k$ holds for all but finitely many terms.
\end{remark}

\begin{remark}
  When the ratio between consecutive terms is simpler than the terms
  themselves, which
  is especially true when the terms are in product form, the
  following test is often more
  convenient to apply.
\end{remark}

\begin{proposition}[Ratio comparison test]
  \phantomsection
  \addcontentsline{toc}{section}{Ratio comparison test}
  Assume $0 \leq a_{k + 1}/a_k \leq b_{k + 1}/b_k$ for all $k \in \NN$.
  \begin{enumerate}
    \item If $\sum_{k = 1}^{\infty} b_k$ converges, then $\sum_{k =
      1}^{\infty} a_k$ converges.
    \item If $\sum_{k = 1}^{\infty} a_k$ diverges, then $\sum_{k =
      1}^{\infty} b_k$ diverges.
  \end{enumerate}
\end{proposition}

\begin{proof}
  By multiplying the inequalities $a_{i + 1}/a_i \leq b_{i + 1}/b_i$
  from $i = 1$ to $i = k - 1$, we obtain
  \[ \frac{a_k}{a_1} \leq \frac{b_k}{b_1} \qquad \text{or} \qquad
  a_k \leq \frac{a_1}{b_1} b_k. \]
  Since the series $\sum_{k = 1}^{\infty} \frac{a_1}{b_1} b_k$ and
  $\sum_{k = 1}^{\infty} b_k$ have the same behavior, the result
  follows directly from \prpref{comparison-test}.
\end{proof}

\begin{definition}[Geometric series]
  \phantomsection
  \addcontentsline{toc}{section}{Geometric series}
  A \vocab{geometric series} is a series of the form
  \[ \sum_{k = 0}^{\infty} ar^k = a + ar + ar^2 + ar^3 + \dots \]
  where $a, r \in \RR$.
\end{definition}

\begin{lemma}
  \lemlabel{precursor-to-geometric-series-test}
  The sequence $(r^n)$ converges to 0 if $r \in (-1, 1)$, converges
  to 1 if $r = 1$, and diverges otherwise.
\end{lemma}

\begin{proof}
  \phantom{.}

  \textbf{Case 1.} For $r > 1$, the sequence $(r^n)$ is strictly
  increasing and unbounded. By monotone convergence
  (\thmref{monotone-convergence}), the sequence diverges to $\infty$.

  \textbf{Case 2.} For $r = 1$, we have $r^n = 1$ for all $n \in
  \NN$. Thus, the sequence converges to 1.

  \textbf{Case 3.} For $0 < r < 1$, the sequence $(r^n)$ is strictly
  decreasing and bounded below by 0. By monotone convergence, the
  sequence converges to 0.

  \textbf{Case 4.} For $r = 0$, we have $r^n = 0$ for all $n \in
  \NN$. Thus, the sequence converges to 0.

  \textbf{Case 5.} For $-1 < r < 0$, we can bound the sequence as follows:
  \[ -\abs{r}^n \leq r^n \leq \abs{r}^n. \]
  Since $0 < \abs{r} < 1$, by Case 3, we have $\lim_{n \to
  \infty} \abs{r}^n = 0$. Thus, $\lim_{n \to \infty} -\abs{r}^n =
  0$ as well. By the sequence squeeze theorem
  (\thmref{sequence-squeeze}), the sequence $(r^n)$ converges to 0.

  \textbf{Case 6.} For $r \leq -1$, the proof similar to the one
  presented in \expref{alternating-minus-one-diverges}.
\end{proof}

\begin{proposition}[Geometric series test]
  \phantomsection
  \addcontentsline{toc}{section}{Geometric series test}
  \prplabel{geometric-series-test}
  Assume $a$ and $r$ are non-zero real numbers. Then
  \begin{align*}
    \sum_{k = 0}^{\infty} ar^k =
    \begin{cases}
      \frac{a}{1 - r} & \text{if $\abs{r} < 1$} \\
      \text{diverges} & \text{if $\abs{r} \geq 1$}.
    \end{cases}
  \end{align*}
\end{proposition}

\begin{proof}
  The case where $\abs{r} > 1$ follows from
  \lemref{precursor-to-geometric-series-test} and
  \thmref{necessary-condition-for-convergence}. When $r = 1$, the
  series $a + a + a + \dots$ which clearly diverges, and when $r =
  -1$ the series $a - a + a - a + a - \dots$ whose sequence of
  partial sums, $(a, 0, a, 0, \dots)$, is clearly not converging. We
  therefore turn to the case that $\abs{r} < 1$. Note that
  \begin{align*}
    (1 - r) (1 + r + r^2 + r^3 + \dots + r^n) & = 1 + r + r^2 + r^3 +
    \dots + r^n \\
    & \phantom{=} \:\:\:\:\, - r - r^2 - r^3 - \dots - r^{n - 1} -
    r^{n + 1} \\
    & = 1 + 0 + 0 + \dots + 0 - r^{n + 1} \\
    & = 1 - r^{n + 1},
  \end{align*}
  which, by dividing over the $1 - r$, shows that
  \[ 1 + r + r^2 + r^3 + \dots + r^n = \frac{1 - r^{n + 1}}{1 - r}, \]
  and hence
  \[ s_n = a + ar + ar^2 + ar^3 + \dots + ar^n = \frac{a(1 - r^{n +
  1})}{1 - r}, \]
  where $s_n$ is the $n$-th partial sum of $\sum_{k = 0}^{\infty} ar^k$. Thus,
  \begin{align*}
    \sum_{k = 0}^{\infty} ar^k & = \lim_{n \to \infty} s_n \\
    & = \lim_{n \to \infty} \frac{a(1 - r^{n + 1})}{1 - r} \\
    & = \frac{a(1 - 0)}{1 - r} && \text{(by
    \lemref{precursor-to-geometric-series-test})} \\
    & = \frac{a}{1 - r}.
  \end{align*}
\end{proof}

\begin{theorem}[Harmonic series diverges]
  \phantomsection
  \addcontentsline{toc}{section}{Harmonic series diverges}
  The harmonic series $\sum_{k = 1}^{\infty} \frac{1}{k}$ diverges.
\end{theorem}

\begin{proof}
  Observe that
  \begin{align*}
    \sum_{k = 1}^{\infty} \frac{1}{k} & = 1 + \frac{1}{2} +
    \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6} +
    \frac{1}{7} + \frac{1}{8} + \dots \\
    & = 1 + \left(\frac{1}{2}\right) + \left(\frac{1}{3} + \frac{1}{4}\right) +
    \left(\frac{1}{5} + \frac{1}{6} + \frac{1}{7} +
    \frac{1}{8}\right) + \dots \\
    & \geq 1 + \left(\frac{1}{2}\right) + \left(\frac{1}{4} +
    \frac{1}{4}\right) +
    \left(\frac{1}{8} + \frac{1}{8} + \frac{1}{8} +
    \frac{1}{8}\right) + \dots \\
    & = 1 + \left(\frac{1}{2}\right) + \left(\frac{1}{2}\right) +
    \left(\frac{1}{2}\right) + \dots
  \end{align*}
  In particular, if $s_n$ is the $n$-th partial sum of the harmonic
  series, then $(s_n)$ is monotonically increasing and, by the above,
  \[ s_{2^n} \geq 1 + n \cdot \frac{1}{2}. \]
  And since $(1 + n \cdot \frac{1}{2})$ diverges to $\infty$, the
  comparison test (\prpref{comparison-test}) the subsequence
  $(s_{2^n})$ diverges to $\infty$. And for a monotonically
  increasing sequence, if a subsequence diverges to $\infty$,
  implying that the entire sequence $(s_n)$ is unbounded, then
  $(s_n)$ is also diverging to $\infty$ by the monotone convergence
  theorem (\thmref{monotone-convergence}). That is, the harmonic
  series diverges to $\infty$.
\end{proof}

Here's another proof that I particularly enjoy.

\begin{proof}
  Suppose for a contradiction that the harmonic series diverges
  converges to $S$. Then
  \begin{align*}
    S & = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} +
    \frac{1}{6} + \frac{1}{7} + \frac{1}{8} + \dots \\
    & = \left(1 + \frac{1}{2}\right) + \left(\frac{1}{3} + \frac{1}{4}\right) +
    \left(\frac{1}{5} + \frac{1}{6}\right) + \left(\frac{1}{7} +
    \frac{1}{8}\right) + \dots \\
    & > \left(\frac{1}{2} + \frac{1}{2}\right) + \left(\frac{1}{4} +
    \frac{1}{4}\right) +
    \left(\frac{1}{6} + \frac{1}{6}\right) + \left(\frac{1}{8} +
    \frac{1}{8}\right) + \dots \\
    & = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} +
    \frac{1}{6} + \frac{1}{7} + \frac{1}{8} + \dots \\
    & = S.
  \end{align*}
  We've shown $S > S$, a contradiction.
\end{proof}

\begin{proposition}[The $p$-series test]
  \phantomsection
  \addcontentsline{toc}{section}{The $p$-series test}
  The series $\sum_{k = 1}^{\infty} \frac{1}{k^p}$ converges if and
  only if $p > 1$.
\end{proposition}

\begin{proof}
  \phantom{.}

  $(\Rightarrow)$ If $p \leq 1$, then
  \[ \frac{1}{k} \leq \frac{1}{k^p} \qquad \text{for all $k \in \NN$}. \]
  And since $\sum_{k = 1}^{\infty} \frac{1}{k}$ diverges, by the
  comparison test (\prpref{comparison-test}), $\sum_{k = 1}^{\infty}
  \frac{1}{k^p}$ diverges too.

  $(\Leftarrow)$ Now assume that $p > 1$. Then
  \begin{align*}
    \sum_{k = 1}^{\infty} \frac{1}{k^p} & = 1 + \frac{1}{2^p} +
    \frac{1}{3^p} + \frac{1}{4^p} + \frac{1}{5^p} + \frac{1}{6^p} +
    \frac{1}{7^p} + \dots \\
    & = 1 + \left(\frac{1}{2^p} + \frac{1}{3^p}\right) +
    \left(\frac{1}{4^p} + \frac{1}{5^p} + \frac{1}{6^p} +
    \frac{1}{7^p}\right) + \dots \\
    & < 1 + \left(\frac{1}{2^p} + \frac{1}{2^p}\right) +
    \left(\frac{1}{4^p} + \frac{1}{4^p} + \frac{1}{4^p} +
    \frac{1}{4^p}\right) + \dots \\
    & = 1 + \left(\frac{2}{2^p}\right) + \left(\frac{4}{4^p}\right) + \dots \\
    & = 1 + \left(\frac{1}{2^{p - 1}}\right) + \left(\frac{1}{4^{p -
    1}}\right) + \dots \\
    & = 1 + \left(\frac{1}{2^{p - 1}}\right) + \left(\frac{1}{2^{2(p
    - 1)}}\right) + \dots \\
    & = 1 + \left(\frac{1}{2^{p - 1}}\right) + \left(\frac{1}{2^{p -
    1}}\right)^2 + \dots \\
    & = 1 + \sum_{k = 0}^{\infty} \left(\frac{1}{2^p}\right)^k.
  \end{align*}
  Finally, note that $\sum_{k = 0}^{\infty} (\frac{1}{2^p})^k$ is a
  geometric series with $\abs{r} = \frac{1}{2^p} < 1$, so by the
  geometric series test (\prpref{geometric-series-test}), this series
  converges. And the above also shows that, term-by-term, this
  geometric series is larger than $\sum_{k = 1}^{\infty}
  \frac{1}{k^p}$. So by the comparison test
  (\prpref{comparison-test}), $\sum_{k = 1}^{\infty}
  \frac{1}{k^p}$ converges too.
\end{proof}

\begin{example}
  The series
  \[ \sum_{n = 2}^{\infty} \frac{1}{n \ln n} \]
  diverges.
\end{example}

\begin{remark}
  The ratio comparison test yields two simple and powerful tests that
  are closely related. Both tests work exclusively with the terms of
  the given series---they require neither an initial guess about
  convergence nor the discovery of a series for comparison.
\end{remark}

\begin{proposition}[Ratio test]
  \phantomsection
  \addcontentsline{toc}{section}{Ratio test}
  \prplabel{ratio-test}
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ r = \lim_{n \to \infty} \frac{a_{n + 1}}{a_n}. \]
  \begin{enumerate}
    \item If $r < 1$, the series $\sum_{n = 1}^{\infty} a_n$ converges.
    \item If $r > 1$, the series $\sum_{n = 1}^{\infty} a_n$ diverges.
    \item If $r = 1$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{proposition}[Root test]
  \phantomsection
  \addcontentsline{toc}{section}{Root test}
  \prplabel{root-test}
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ \rho = \lim_{n \to \infty} \sqrt[n]{a_n}. \]
  \begin{enumerate}
    \item If $\rho < 1$, the series $\sum_{n = 1}^{\infty} a_n$ converges.
    \item If $\rho > 1$, the series $\sum_{n = 1}^{\infty} a_n$ diverges.
    \item If $\rho = 1$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{example}
  Let $a$ and $b$ be distinct positive numbers. Determine the convergence of
  \[ 1 + a + ab + a^2b + a^2b^2 + \dots + a^nb^{n - 1} + a^nb^n + \dots. \]
\end{example}

\begin{definition}[Strength of convergence tests]
  \phantomsection
  \addcontentsline{toc}{section}{Strength of convergence tests}
  We consider the strength of convergence tests to be its ability to
  yield a result. That is, for one convergence test $A$ to be
  \vocab{stronger than} another test $B$, the weaker test $B$
  giving a result implies that the stronger test $A$ also yields a
  result. It must also be true that there exists a case where the
  stronger convergence test $A$ works but the weaker test $B$ does not.
\end{definition}

\begin{proposition}[Root test is stronger than ratio test]
  \phantomsection
  \addcontentsline{toc}{section}{Root test is stronger than ratio test}
  Let $(a_n)$ be a positive sequence. Suppose that $r$ and $\rho$ are
  given by \prpref{ratio-test} and \prpref{root-test}, respectively.
  If $r$ exists, then $\rho$ exists and
  \[ r = \lim_{n \to \infty} \frac{a_{n + 1}}{a_n} = \lim_{n \to
  \infty} \sqrt[n]{a_n} = \rho. \]
  The converse, however, is not necessarily true.
\end{proposition}

\begin{proposition}[Alternating series test]
  \phantomsection
  \addcontentsline{toc}{section}{Alternating series test}
  Assume that $(a_k)$ is a monotonically decreasing sequence and $a_k
  \to 0$. Then $\sum_{k = 1}^{\infty} (-1)^{k + 1} a_k$ converges.
\end{proposition}

\begin{example}[Hardy \cite{hardy1949divergent}]
  The alternating series
  \[ \sum_{n = 2}^{\infty} \frac{(-1)^n}{\sqrt{n} + (-1)^n} \]
  diverges.
\end{example}

\begin{proposition}[Absolute convergence test]
  \phantomsection
  \addcontentsline{toc}{section}{Absolute convergence test}
  \prplabel{absolute-convergence-test}
  If $\sum_{k = 1}^{\infty} \abs{a_k}$ converges, then $\sum_{k =
  1}^{\infty} a_k$ converges too.
\end{proposition}

\begin{definition}[Absolute convergence]
  \phantomsection
  \addcontentsline{toc}{section}{Absolute convergence}
  A series $\sum_{k = 1}^{\infty} a_k$ is said to \vocab{converge
  absolutely} if $\sum_{k = 1}^{\infty} \abs{a_k}$ converges.
\end{definition}

\begin{definition}[Conditional convergence]
  \phantomsection
  \addcontentsline{toc}{section}{Conditional convergence}
  A series $\sum_{k = 1}^{\infty} a_k$ is said to \vocab{converge
  conditionally} if it converges but does not converge absolutely.
\end{definition}

\begin{example}
  Determine the convergene of
  \[ \sum_{n = 1}^{\infty} \frac{1}{2^{\sqrt{n}}}. \]
  converges.
\end{example}

\begin{remark}
  Historically, the most interesting series mathematicians were
  encountering in the early 19th Century all fell into the
  inconclusive category of the above tests. To overcome the principal
  drawback of these tests, various sophisticated and finer tests were
  developed. For example, Gauss's test (\prpref{gauss-test})
  completely sets the convergence for all hypergeometric series.
  Kummer's test (\prpref{kummer-test}) gives characterizations for
  convergence or divergence of all positive series. Cauchy's
  condensation test (\prpref{cauchy-condensation-test}) provides an
  approach to study the convergence or divergence of a class of
  series via a rather ``thin'' subsequence. To demonstrate the
  progression of these sophisticated and finer tests, we will first
  introduce the Raabe (\prpref{raabe-test}) and logarithmic
  (\prpref{logarithmic-test}) tests. Then, we present the Kummer
  (\prpref{kummer-test}) and Gauss (\prpref{gauss-test}) tests. For
  series with monotonically decreasing terms, we formulate a few
  simple and interesting tests based on Cauchy's condensation test
  (\prpref{cauchy-condensation-test}). Finally, we show that no
  single test can determine convergence or divergence for all positive series.
\end{remark}

\begin{proposition}[Raabe's test]
  \phantomsection
  \addcontentsline{toc}{section}{Raabe's test}
  \prplabel{raabe-test}
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ \mathcal{R}_n = n \left(1 - \frac{a_{n + 1}}{a_n}\right). \]
  \begin{enumerate}
    \item If $\mathcal{R}_n \geq r > 1$, the series $\sum_{n =
      1}^{\infty} a_n$ converges.
    \item If $\mathcal{R}_n \leq 1$, the series $\sum_{n =
      1}^{\infty} a_n$ diverges.
  \end{enumerate}
\end{proposition}

\begin{proposition}[Raabe's limit test]
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ \mathcal{R} = \lim_{n \to \infty} \mathcal{R}_n = \lim_{n \to
  \infty} n \left(1 - \frac{a_{n + 1}}{a_n}\right). \]
  \begin{enumerate}
    \item If $\mathcal{R} > 1$, the series $\sum_{n = 1}^{\infty}
      a_n$ converges.
    \item If $\mathcal{R} < 1$, the series $\sum_{n = 1}^{\infty} a_n$ diverges.
    \item If $\mathcal{R} = 1$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{example}
  The series
  \[ \sum_{n = 1}^{\infty} \frac{1}{2^{\sqrt{n}}} \]
  converges.
\end{example}

\begin{example}
  Determine the convergence of
  \[ \sum_{n = 1}^{\infty} \frac{1}{n!} \left(\frac{n}{e}\right)^n. \]
\end{example}

\begin{proposition}[Raabe's limit test corollary]
  \prplabel{raabe-limit-test-cor}
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ \mathcal{R}^\ast = \lim_{n \to \infty} \left(\frac{a_{n +
  1}}{a_n}\right)^n. \]
  \begin{enumerate}
    \item If $\mathcal{R}^\ast < 1/e$, the series $\sum_{n = 1}^{\infty}
      a_n$ converges.
    \item If $\mathcal{R}^\ast > 1/e$, the series $\sum_{n =
      1}^{\infty} a_n$ diverges.
    \item If $\mathcal{R}^\ast = 1/e$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{example}
  Using \prpref{raabe-limit-test-cor}, show that the series
  \[ \sum_{n = 1}^{\infty} \frac{1}{2^{\sqrt{n}}} \]
  converges.
\end{example}

\begin{proposition}[Logarithmic test]
  \phantomsection
  \addcontentsline{toc}{section}{Logarithmic test}
  \prplabel{logarithmic-test}
  Assume $a_n > 0$ for all $n \in \NN$. Define
  \[ \mathcal{L}_n = \frac{\ln(1/a_n)}{\ln n}. \]
  \begin{enumerate}
    \item If $\mathcal{L}_n \geq 1 > 1$, the series converges.
    \item If $\mathcal{L}_n \leq 1$, the series diverges.
  \end{enumerate}
\end{proposition}

\begin{example}
  Let $p > 1$. Determine the convergence of
  \[ \sum_{n = 1}^{\infty} \frac{\ln n}{n^p}. \]
\end{example}

\begin{proposition}[Logarithmic test is stronger than Raabe's test]
  \phantomsection
  \addcontentsline{toc}{section}{Logarithmic test is stronger than Raabe's test}
  Let $(a_n)$ be a positive sequence. Suppose that $\mathcal{R}_n$
  and $\mathcal{L}_n$ are given by \prpref{raabe-test} and
  \prpref{logarithmic-test}, respectively. If $\lim_{n \to \infty}
  \mathcal{R}_n$ exists, finite or infinite, then $\lim_{n \to
  \infty} \mathcal{L}_n$ exists and
  \[ \lim_{n \to \infty} \mathcal{R}_n = \lim_{n \to \infty} \mathcal{L}_n. \]
  The converse, however, is not necessarily true.
\end{proposition}

\begin{proposition}[Kummer's test]
  \phantomsection
  \addcontentsline{toc}{section}{Kummer's test}
  \prplabel{kummer-test}
  Assume $a_n > 0$ for all $n \in \NN$.
  \begin{enumerate}
    \item The series $\sum_{n = 1}^{\infty} a_n$ converges if there
      is a positive sequence $(c_n)$ and some constant $k > 0$ such that
      \[ \mathcal{K}_n = c_n - \frac{a_{n + 1}}{a_n} c_{n + 1} \geq k. \]
    \item The series $\sum_{n = 1}^{\infty} a_n$ diverges if there is
      a positive sequence $(c_n)$ such that $\sum_{n = 1}^{\infty} 1/c_n$
      diverges and $\mathcal{K}_n \leq 0$.
  \end{enumerate}
\end{proposition}

\begin{proposition}[Kummer's limit test]
  Assume $a_n > 0$ for all $n \in \NN$. For a positive sequence $(a_n)$, define
  \[ \kappa = \lim_{n \to \infty} \left(c_n - \frac{a_{n + 1}}{a_n}
  c_{n + 1}\right). \]
  \begin{enumerate}
    \item If $\kappa > 0$, then the series $\sum_{n = 1}^{\infty}$ converges.
    \item If $\kappa < 0$ and $\sum_{n = 1}^{\infty} 1/c_n$ diverges,
      then the series $\sum_{n = 1}^{\infty}$ diverges.
    \item If $\kappa = 0$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{remark}
  The drawback of Kummer's test is the choice of $c_n$, which has to
  come from the user, and usually there is no particular motivation.
  Thus, using Kummer's test is equally as difficult as using the
  comparison test.

  However, in contrast to all tests we have studied, Kummer's test does not
  merely give very powerful sufficient conditions for convergence or
  divergence of a positive series. Surprisingly, the conditions (1)
  and (2) in the test are indeed necessary. This fact is not well publicized.
\end{remark}

\begin{theorem}[Kummer's characterization of convergent series]
  \phantomsection
  \addcontentsline{toc}{section}{Kummer's characterization of convergent series}
  If the series $\sum_{n = 1}^{\infty} a_n$ converges, then there
  exists a positive sequence $(c_n)$ for which
  \[ \mathcal{K}_n = c_n - \frac{a_{n + 1}}{a_n} c_{n + 1} \geq 1. \]
\end{theorem}

\begin{theorem}[Kummer's characterization of divergent series]
  \phantomsection
  \addcontentsline{toc}{section}{Kummer's characterization of divergent series}
  If the series $\sum_{n = 1}^{\infty} a_n$ diverges, then there
  exists a positive sequence $(c_n)$ for which
  $\sum_{n = 1}^{\infty} 1/c_n$ diverges and
  \[ \mathcal{K}_n = c_n - \frac{a_{n + 1}}{a_n} c_{n + 1} \leq 0. \]
\end{theorem}

\begin{proposition}[Bertrand's test]
  \phantomsection
  \addcontentsline{toc}{section}{Bertrand's test}
  Define
  \[ \mathcal{B}_n = \ln n (\mathcal{R}_n - 1) = \ln n \left(n
      \left(1 - \frac{a_{n +
  1}}{a_n}\right) - 1\right). \]
  Suppose that
  \[ \mathcal{B} = \lim_{n \to \infty} \mathcal{B}_n. \]
  \begin{enumerate}
    \item If $\mathcal{B} > 1$, then the positive series $\sum_{n = 1}^{\infty}
      a_n$ converges.
    \item If $\mathcal{B} < 1$, then the positive series $\sum_{n = 1}^{\infty}
      a_n$ diverges.
  \end{enumerate}
\end{proposition}

\begin{proposition}[Gauss's test]
  \phantomsection
  \addcontentsline{toc}{section}{Gauss's test}
  \prplabel{gauss-test}
  Let $\sum_{n = 1}^{\infty} a_n$ be a positive series. Suppose
  \[ \frac{a_{n + 1}}{a_n} = 1 - \frac{\mu}{n} + \frac{\theta_n}{n^k} \]
  where $\theta_n$ is a bounded sequence and $k > 1$.
  \begin{enumerate}
    \item If $\mu > 1$, the series converges.
    \item If $\mu \leq 1$, the series diverges.
  \end{enumerate}
\end{proposition}

\begin{example}
  Check the convergence of the \textit{hypergeometric series}
  \[ F(\alpha, \beta, \gamma) = 1 + \sum_{n = 1}^{\infty}
    \frac{\alpha (\alpha + 1) \dots (\alpha + n - 1) \beta (\beta + 1)
  \dots (\beta + n - 1)}{n! \gamma (\gamma + 1) \dots (\gamma + n - 1)}. \]
\end{example}

\begin{example}
  Consider the convergence of
  \[ \sum_{n = 2}^{\infty} \frac{1}{n \ln n} \]
  using Gauss's test.
\end{example}

\begin{proposition}[Cauchy's condensation test]
  \phantomsection
  \addcontentsline{toc}{section}{Cauchy's condensation test}
  \prplabel{cauchy-condensation-test}
  Let $(a_n)$ be a monotononically decreasing sequence where $a_n
  > 0$ for all $n \in \NN$. Then $\sum_{n = 1}^{\infty} a_n$
  converges if and only if $\sum_{n = 1}^{\infty} 2^n a_{2^n}$ converges.
\end{proposition}

\begin{proposition}[Generalization of Cauchy's condensation test]
  Let $(a_n)$ be a monotonically decreasing sequence where $a_n >
  0$ for all $n \in \NN$. If there is a strictly increasing sequence
  $1 \leq n_1 < n_2 < \dots$ such that $(n_{k + 1} - n_k) / (n_{k} -
  n_{k - 1})$ is bounded as a function of $k$, then $\sum_{n =
  1}^{\infty} a_n$ converges if and only if $\sum_{k = 1}^{\infty}
  (n_{k + 1} - n_k) a_{n_k}$ converges.
\end{proposition}

\begin{example}
  Consider the convergence of
  \[ \sum_{n = 2}^{\infty} \frac{1}{n \ln n} \]
  using Cauchy's condensation test.
\end{example}

\begin{proposition}
  Let $(a_n)$ be a monotonically decreasing sequence where $a_n > 0$
  for all $n \in \NN$. Define
  \[ \rho = \lim_{n \to \infty} \frac{a_{2n}}{a_n}. \]
  \begin{enumerate}
    \item If $\rho < 1/2$, then the series $\sum_{n = 1}^{\infty}
      a_n$ converges.
    \item If $\rho > 1/2$, then the series $\sum_{n = 1}^{\infty} a_n$ diverges.
    \item If $\rho = 1/2$, the test is inconclusive.
  \end{enumerate}
\end{proposition}

\begin{example}
  Let $p > 0$. Study the convergence of
  \[ \sum_{n = 1}^{\infty} \frac{\ln n}{n^p}. \]
\end{example}

\begin{definition}[Rates of convergence and divergence]
  \phantomsection
  \addcontentsline{toc}{section}{Rates of convergence and divergence}
  Consider two positive series
  \[ \sum_{n = 1}^{\infty} a_n \qquad \text{and} \qquad \sum_{n =
  1}^{\infty} a_n^\ast. \]
  \begin{enumerate}
    \item Suppose both series converge. Let
      \[ R_n = \sum_{i = n + 1}^{\infty} a_i \qquad \text{and} \qquad
      R_n^\ast = \sum_{i = n + 1}^{\infty} a_i^\ast. \]
      We say that $\sum_{n = 1}^{\infty} a_n^\ast$ \vocab{converges more
      slowly} than $\sum_{n = 1}^{\infty} a_n$ if
      \[ \lim_{n = \infty} \frac{R_n}{R_n^\ast} = 0. \]
    \item Suppose both series diverge. Let
      \[ S_n = \sum_{i = 1}^{n} a_i \qquad \text{and} \qquad S_n^\ast
      = \sum_{i = 1}^{n} a_i^\ast. \]
      We say that $\sum_{n = 1}^{\infty} a_n^\ast$ \vocab{diverges more
      slowly} than $\sum_{n = 1}^{\infty} a_n$ if
      \[ \lim_{n \to \infty} \frac{S_n^\ast}{S_n} = 0. \]
  \end{enumerate}
\end{definition}

\begin{lemma}
  Let $\sum_{n = 1}^{\infty} a_n$ be a positive series.
  \begin{enumerate}
    \item If $\sum_{n = 1}^{\infty} a_n$ converges, then there is a
      monotone sequence $(b_n)$ such that \\
      $\lim_{n \to \infty} b_n = \infty$ and the series $\sum_{n =
      1}^{\infty} a_n b_n$ converges.
    \item If $\sum_{n = 1}^{\infty} a_n$ diverges, then there is a
      monotone sequence $(b_n)$ such that \\
      $\lim_{n \to \infty} b_n = 0$ and the series $\sum_{n =
      1}^{\infty} a_n b_n$ diverges.
  \end{enumerate}
\end{lemma}

\begin{theorem}[Nonexistence of the universal test]
  \phantomsection
  \addcontentsline{toc}{section}{Nonexistence of the universal test}
  There is no universal comparison test that can decide the
  convergence or divergence of all positive series.
\end{theorem}

\begin{remark}
  Under the absolute convergence test
  (\prpref{absolute-convergence-test}), all of the preceding tests
  for positive series become tests for absolute convergence of
  general series. But none of the convergence tests that we have
  examined so far are applicable to a series that is not alternating
  and does not converge absolutely. We now present two tests that are
  applicable to the general series of the form
  \[ \sum_{n = 1}^{\infty} a_n b_n = a_1 b_1 + a_2 b_2 + a_3 b_3 + \dots. \]
  Both tests are based on the following algebraic manipulation, which
  is often called the \textit{Abel's summation formula}.
\end{remark}

\begin{theorem}[Abel's summation formula]
  \phantomsection
  \addcontentsline{toc}{section}{Abel's summation formula}
  Let $(a_n)$ and $(b_n)$ be arbitrary sequences. Define $(s_n)$ to
  be the sequence of partial sums of $(a_n)$. That is, $s_n = \sum_{k
  = 1}^{n} a_k$. Then
  \[ \sum_{k = 1}^{n} a_k b_k = \sum_{k = 1}^{n - 1} s_k (b_k - b_{k
  + 1}) + s_n b_n. \]
\end{theorem}

\begin{proposition}[Abel's test]
  \phantomsection
  \addcontentsline{toc}{section}{Abel's test}
  Let $\sum_{n = 1}^{\infty} a_n$ be a convergent series, and let
  $(b_n)$ be a bounded and monotone sequence. Then $\sum_{n =
  1}^{\infty} a_n b_n$ converges.
\end{proposition}

\begin{proposition}[Dirichlet's test]
  \phantomsection
  \addcontentsline{toc}{section}{Dirichlet's test}
  Let $\sum_{n = 1}^{\infty} a_n$ be a bounded series, and let
  $(b_n)$ be a positive and monotonically decreasing sequence with
  $\lim_{n \to \infty} b_n = 0$. Then $\sum_{n = 1}^{\infty} a_n b_n$ converges.
\end{proposition}

\begin{example}
  Determine the convergence of
  \[ \sum_{n = 1}^{\infty} b_n \sin(nx) \qquad \text{and} \qquad
  \sum_{n = 1}^{\infty} b_n \cos(nx) \]
  where $(b_n)$ is positve, monotonically decreasing, and converges to zero.
\end{example}

\begin{proposition}[Comparison test for general series]
  \phantomsection
  \addcontentsline{toc}{section}{Comparison test for general series}
  Let $\sum_{n = 1}^{\infty} a_n$ and $\sum_{n = 1}^{\infty} b_n$ satisfy
  \[ \text{$\lim_{n \to \infty} \frac{a_n}{b_n} = c_n$ and
  $\frac{a_n}{b_n}$ is monotone.} \]
  Then
  \begin{enumerate}
    \item If $\sum_{n = 1}^{\infty} b_n$ converges, then $\sum_{n =
      1}^{\infty} a_n$ converges.
    \item If $c \neq 0$, then both series are either both divergent,
      both conditionally convergent, or both absolutely convergent.
  \end{enumerate}
\end{proposition}

\begin{theorem}
  \phantomsection
  \addcontentsline{toc}{section}{The associative law holds for
  convergent series}
  The associative law holds for convergent series.
\end{theorem}

\begin{definition}[Rearrangements of series]
  \phantomsection
  \addcontentsline{toc}{section}{Rearrangements of series}
  A \vocab{rearrangement} of a series $\sum_{n = 1}^{\infty} a_n$ is
  a series $\sum_{n = 1}^{\infty} b_n$ for which there is a bijection
  $f : \NN \to \NN$ such that $a_n = b_{f(n)}$.
\end{definition}

\begin{theorem}
  If the series $\sum_{n = 1}^{\infty} a_n$ converges absolutely,
  then any rearrangement of this series converges to the same limit.
\end{theorem}

\begin{corollary}
  \phantomsection
  \addcontentsline{toc}{section}{The commutative law holds for
  absolutely convergent series}
  The commutative law holds for absolutely convergent series.
\end{corollary}

\begin{theorem}[Riemann series theorem]
  \phantomsection
  \addcontentsline{toc}{section}{Riemann series theorem}
  If the series $\sum_{n = 1}^{\infty} a_n$ converges conditionally,
  then there exists a rearrangement of this series that either
  converges to any arbitrary real number or diverges to $\pm \infty$.
\end{theorem}

\begin{definition}[Finite decimal representations of real numbers]
  \phantomsection
  \addcontentsline{toc}{section}{Finite decimal representations of real numbers}
  A real number of the form
  \[ x = a_0 + \frac{a_1}{10} + \frac{a_2}{10^2} + \dots + \frac{a_n}{10^n}, \]
  where $a_0$ is a nonnegative integer and $a_1, \dots, a_n$ are
  integers satisfying $0 \leq a_i \leq 9$ and is usually written more briefly as
  \[ x = a_0. a_1 a_2 \ldots a_n. \]
  This is said to be the \vocab{finite decimal representation} of $x$.
\end{definition}

\begin{proposition}
  Real numbers with a finite decimal representation are necessary rational.
\end{proposition}

\begin{proof}
  Trivial.
\end{proof}

\begin{proposition}[Finite decimal approximations of real numbers]
  \phantomsection
  \addcontentsline{toc}{section}{Finite decimal approximations of real numbers}
  \prplabel{finite-decimal-appox-real}
  Assume $x$ is a nonnegative real number.
  \begin{enumerate}
    \item Then there is a finite decimal $r_n = a_0. a_1 a_2 \ldots
      a_n$ such that
      \[ r_n \leq x < r_n + \frac{1}{10^n} \qquad \text{for all $n \in \NN$}. \]
    \item Then there is a finite decimal $s_n = b_0. b_1 b_2 \ldots
      b_n$ such that
      \[ s_n < x \leq s_n + \frac{1}{10^n} \qquad \text{for all $n \in \NN$}. \]
  \end{enumerate}
\end{proposition}

\begin{proof}
  We proceed with a proof of part 1. Let $S$ be the set of
  nonnegative integers less than or equal to $x$. Then $S$ is
  nonempty since $0 \in S$, and $S$ is bounded above by $x$. By the
  completeness of $\RR$, $S$ has a supremum, which we denote as $a_0
  = \sup(S)$. Since $S$ contains only integers and has $a_0$ as its
  least upper bound, $a_0$ must itself be an integer in $S$. That is,
  $a_0 = \floor{x}$. This gives us our first approximation:
  \[ a_0 \leq x < a_0 + 1. \tag{1} \]

  For the first decimal digit, we define $a_1 = \floor{10x - 10a_0}$.
  This gives us
  \[ a_1 \leq 10x - 10a_0 < a_1 + 1. \tag{2} \]
  From (1) and (2), we can deduce that $0 \leq a_1 \leq 9$, confirming
  that $a_1$ is indeed a single decimal digit. Rearranging (2)
  gives us our second approximation:
  \[ a_0 + \frac{a_1}{10} \leq x < a_0 + \frac{a_1}{10} + \frac{1}{10}. \]

  This process extends naturally to subsequent decimal places.
  After selecting digits $a_1, a_2, \dots, a_{n - 1}$ with $0 \leq a_i
  \leq 9$ for each $i$, we define
  \[ a_n = \floor{10^n x - (10^n a_0 + 10^{n - 1} a_1 + \dots + 10
  a_{n - 1})}. \]
  This gives us
  \[ a_n \leq 10^n x - (10^n a_0 + 10^{n - 1} a_1 + \dots + 10 a_{n -
  1}) < a_n + 1. \tag{3} \]
  Rearranging (3) gives us the following approximation:
  \[ a_0 + \frac{a_1}{10} + \frac{a_2}{10^2} + \dots +
    \frac{a_n}{10^n} \leq x < a_0 + \frac{a_1}{10} + \frac{a_2}{10^2} +
  \dots + \frac{a_n}{10^n} + \frac{1}{10^n}. \]
  By construction, observe that $0 \leq a_n \leq 9$ for each $n$, and so we have
  \[ r_n \leq x < r_n + \frac{1}{10^n}, \]
  where $r_n = a_0. a_1 a_2 \ldots a_n$ represents our finite decimal
  approximation of $x$ with $n$ decimal places. This completes the
  proof of part 1.

  The proof of part 2 is quite similar.
\end{proof}

\begin{theorem}[Infinite decimal representations of real numbers]
  \phantomsection
  \addcontentsline{toc}{section}{Infinite decimal representations of
  real numbers}
  Every real number admits at least one infinite decimal representation.
\end{theorem}

\begin{proof}
  First, let's consider any nonnegative real number $x$. By
  \prpref{finite-decimal-appox-real}, for each $n \in \mathbb{N}$,
  there exists a finite decimal $r_n = a_0.a_1a_2\ldots a_n$ such that
  \[ r_n \leq x < r_n + \frac{1}{10^n}, \]
  where $a_0 = \floor{x}$ and each $a_i$ ($1 \leq i \leq n$) is a
  digit between $0$ and $9$.

  Let's define the infinite decimal representation $r = a_0.a_1 a_2
  a_3 \ldots$ as the series
  \[ r = a_0 + \frac{a_1}{10} + \frac{a_2}{10^2} + \frac{a_3}{10^3} +
  \cdots = a_0 + \sum_{i=1}^{\infty} \frac{a_i}{10^i}. \]
  We claim that $r = x$. To prove this, we need to show that $|r - x| = 0$.

  Choose an arbitrary natural number $N$. By our construction, we have
  \[ r_N \leq x < r_N + \frac{1}{10^N}. \tag{1} \]

  Since all digits $a_i$ are between $0$ and $9$, we have
  \begin{align*}
    r - r_N = \sum_{i=N+1}^{\infty} \frac{a_i}{10^i} & \leq
    \sum_{i=N+1}^{\infty} \frac{9}{10^i} \\
    & = \frac{9}{10^{N+1}} \cdot \frac{1}{1-\frac{1}{10}} &&
    \text{(by \prpref{geometric-series-test})} \\
    & = \frac{1}{10^N}
  \end{align*}
  which implies that
  \[ r_N \leq r \leq r_N + \frac{1}{10^N}. \tag{2} \]

  Combining (1) and (2) gives us
  \[ -\frac{1}{10^N} \leq r - x \leq \frac{1}{10^N} \qquad \text{or}
  \qquad \abs{r - x} \leq \frac{1}{10^N}. \]

  Since $N$ was arbitrary, by the Archimedean principle
  (\lemref{archimedean-principle}), we must have $\abs{r - x} = 0$,
  which means that $r = x$.

  For negative real numbers, we simply prepend a negative sign to
  obtain $-a_0. a_1 a_2 a_3 \ldots$ where the $a_i$ are determined from
  $\abs{x}$ as above.
\end{proof}

\begin{theorem}[$0.999\ldots = 1$]
  \phantomsection
  \addcontentsline{toc}{section}{$0.999\ldots = 1$}
  $0.999\ldots$ is an infinite decimal representation of the real number 1.
\end{theorem}

\begin{proof}
  Consider the real number $x = 1$. By \prpref{finite-decimal-appox-real},
  for each $n \in \NN$, there exists a finite decimal $s_n = b_0. b_1
  b_2 \ldots b_n$ such that
  \[ s_n < x \leq s_n + \frac{1}{10^n} \]

  For $x = 1$, we can identify that $b_0 = 0$ and $b_i = 9$ for all
  $i \geq 1$. Thus,
  \[ s_n = 0.\underbrace{99\ldots9}_{\text{n times}} < 1 \leq
  0.\underbrace{99\ldots9}_{\text{n times}} + \frac{1}{10^n}. \]

  Define the infinite decimal $s = 0.999\ldots$ as the series
  \[ s = \frac{9}{10} + \frac{9}{10^2} + \frac{9}{10^3} + \dots =
  \sum_{i=1}^{\infty}\frac{9}{10^i}. \]

  Choose an arbitrary natural number $N$. We have
  \[ s_N < 1 \leq s_N + \frac{1}{10^N} \tag{1} \]
  and
  \[ s - s_N = \sum_{i=N+1}^{\infty}\frac{9}{10^i} =
  \frac{9}{10^{N+1}} \cdot \frac{1}{1-\frac{1}{10}} = \frac{1}{10^N}. \tag{2} \]

  Combining (1) and (2), we get
  \[ s_N < 1 \leq s_N + \frac{1}{10^N} = s. \]

  Since $N$ was arbitrary, we have $s \geq 1$. Obviously, $s \leq 1$
  as well, hence $s = 1$.
\end{proof}

\begin{remark}
  Many struggle to accept that $0.999\ldots = 1$. Some believe
  numbers must have unique decimal representations, making two
  different-looking expressions for the same value seem
  contradictory. Others misinterpret the notion of limits, viewing
  $0.999\ldots$ as an ongoing process that forever approaches 1
  without reaching it, rather than as the fixed value that is the
  limit. These misconceptions stem from intuitive but imprecise
  understandings of infinity and limits. Within the standard definition
  of real numbers, $0.999\ldots$ represents the limit of some
  sequence and equals exactly 1---not approximately, not infinitely
  close, but identical in value.
\end{remark}
