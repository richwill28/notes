\documentclass[11pt,twoside=off,numbers=noenddot]{scrbook}

\input{preamble}
\input{macros}

\title{A Potpourri of Ideas in Theoretical Computer Science}
\author{Richard Willie}

\begin{document}

\maketitle

\chapter*{Preface}
These notes are a loose amalgamation of ideas, concepts, and
explanations drawn from various books, papers, and personal
reflections. They were originally meant for my own understanding and
organization of thoughts, and as such, they may be unpolished,
incomplete, or even occasionally incorrect.

I share them in the hope that they may serve as a useful reference,
but they should not be treated as a primary source of learning.
Readers are strongly encouraged to consult original texts and
authoritative resources for a more rigorous and accurate treatment of
the topics discussed.

Use these notes as a companion to your studies, not as a substitute
for the depth and clarity provided by well-established literature.

\tableofcontents

\newpage

\chapter{Monadic Second-Order Theory of Words}
In this chapter, we look into the expressive power of monadic
second-order logic when the class of structures is restricted to
words, i.e. \emph{word structures}.

\section{Recap: second-order logic}
Second-order logic is an extension of first-order logic. Recall that
one of the main features of first-order logic over propositional
logic, was the ability to quantify over elements that are in the
universe of the structure. Second-order logic not only allows one to
quantify over elements of the universe, but in addition, also allows
quantifying relations over the universe.

To help illustrate the distinction between first-order and
second-order logic, it's helpful to consider an example from number
theory. Here, the objects of our study are the natural numbers $0, 1,
2, \dots$ and their arithmetic. Using first-order logic, we can make
statements about these numbers through atomic expressions such as $x
= y$, $x + y = z$, and $x \times y = z$. These expressions are
combined with logical connectives like $\wedge$, $\neg$, $\vee$, and
$\rightarrow$, as well as quantifiers $\forall x$ and $\exists x$,
where the variables $x, y, z, \dots$ range over the natural numbers.

In second-order logic, we can express even more complex ideas. In
addition to variables like $x, y, z, \dots$ that represent numbers,
second-order logic introduces variables $X, Y, Z, \dots$ that
represent properties or relations between numbers. For these
second-order variables, we also have quantifiers $\forall X$ and
$\exists X$. This means we can now quantify over properties or
relations, not just individual elements. Moreover, in addition to
first-order atomic expressions, we have new atomic expressions of the
form $X(y_1, \dots, y_n)$.

\subsection{Syntax and semantics}
Like first-order logic, second-order logic is defined over a \emph{signature}.

\begin{definition}[Signatures in second-order logic]
  A \emph{signature} is $\tau = \set{\consts, \funcs, \rels}$, where
  \begin{enumerate}
    \item $\consts = \set{c_1, c_2, \dots}$ is a set of constant symbols,
    \item $\funcs = \set{f_1^1, f_2^1, \dots, f_1^2, f_2^2, \dots,
      f_j^k, \dots}$ is a set of function symbols, where the
      superscript indicates the arity of the function, and
    \item $\rels = \set{R_1^1, R_2^1, \dots, R_1^2, R_2^2, \dots,
      R_j^k, \dots}$ is a set of relation symbols, where the
      superscript indicates the arity of the relation.
  \end{enumerate}
\end{definition}

Second-order logic has several kinds of \emph{variables}. It has
\emph{individual} or \emph{first-order variables} denoted by lower
case letters $x, y, z, \dots$ possibly with subscripts. It has
\emph{property} or \emph{relation variables} denoted by uppercase
letters $X, Y, X, \dots$ possibly with subscripts. Finally, it has
\emph{function variables} denoted by uppercase letters $F, G, H,
\dots$ possibly with subscripts. Each relation and function variable
has an arity, which is a positive natural number. We identify
property variables with 1-ary relation variables.

\begin{remark}
  It is noteworthy that although we have property variables, we do
  not have variables for properties of properties. Such variables
  would be part of the formalism of third-order logic.
\end{remark}

Formulas in second-order logic over signature $\tau = (\consts,
\rels)$ are sequences of symbols, where each symbol is one of the following.
\begin{enumerate}
  \item The symbol $\bot$, called \emph{false}.
  \item The symbol $=$, called \emph{equality}.
  \item An element of the infinite set $\vars_1 = \set{x_1, x_2,
    \dots}$ of (first-order) \emph{variables}.
  \item An element of the infinite set $\vars_2 = \set{X_1^1, X_2^1,
    \dots, X_1^2, X_2^2, \dots, X_j^k, \dots}$ of \emph{relational
    variables}, where the superscript indicates the arity of the variable.
  \item Constant symbols and relation symbols in $\tau$.
  \item The symbol $\neg$, called \emph{negation}.
  \item The symbol $\vee$, called \emph{disjunction}.
  \item The symbol $\exists$, called the \emph{existential quantifier}.
  \item The symbols $($ and $)$, called \emph{parentheses}.
\end{enumerate}

\begin{remark}
  For simplicity, we restrict the signature $\tau$ in the definition
  above to include only constant symbols and relation symbols. In
  other words, we exclude function symbols and function variables as,
  after all, functions are just special kinds of relations. In
  addition, we also omit certain logical symbols, such as $\wedge$
  (conjunction), $\rightarrow$ (implication), and the universal
  quantifier $\forall$.
\end{remark}

\begin{abuse}
  We will, however, use the logical symbols $\wedge$, $\rightarrow$,
  and $\forall$ when it's convenient. We appeal to the fact that
  these symbols can be defined in terms of the others.
\end{abuse}

As always, not all such sequences are formulas; only
\emph{well-formed} are formulas in the logic. In order to define
well-formed formulas, we first define the set of \emph{terms}.

\begin{definition}[Terms in second-order logic]
  A \emph{term} $t$ over signature $\tau = (\consts, \rels)$ is either
  \begin{enumerate}
    \item a constant symbol $c \in \consts$, or
    \item a variable $x \in \vars_1$.
  \end{enumerate}
\end{definition}

Having defined terms, we can use them to define well-formed formulas.

\begin{definition}[Well-formed second-order formulas]
  A \emph{well-formed formula (wff)} over signature $\tau = (\consts,
  \rels)$ is inductively defined as follows.
  \begin{enumerate}
    \item $\bot$ is a wff.
    \item If $t_1$ and $t_2$ are terms, then $t_1 = t_2$ is a wff.
    \item If $t_1, \dots, t_k$ are terms and $R^k$ is a relational
      symbol in $\rels$, then $R^k(t_1, \dots, t_k)$ is a wff.
    \item If $t_1, \dots, t_k$ are terms and $X^k$ is a relational
      variable in $\vars_2$, then $X^k(t_1, \dots, t_k)$ is a wff.
    \item If $\phi$ is a wff, then $(\neg \phi)$ is a wff.
    \item If $\phi_1$ and $\phi_2$ are wffs, then $(\phi_1 \vee
      \phi_2)$ are wffs.
    \item If $\phi$ is a wff and $x$ is a variable in $\vars_1$, then
      $(\exists x \phi)$ is a wff.
    \item If $\phi$ is a wff and $X^k$ is a relational variable in
      $\vars_2$, then $(\exists X^k \phi)$ is a wff.
  \end{enumerate}
\end{definition}

More succinctly, we could capture the above definitions of terms and
well-formed formulas by the following BNF grammar,
\begin{align*}
  t & ::= c \mid x \\
  \phi & ::= \bot \mid t = t \mid R(t, \dots, t) \mid X(t, \dots, t)
  \mid (\neg \phi) \mid (\phi \vee \phi) \mid (\exists x \phi) \mid
  (\exists X \phi)
\end{align*}
where $c$ is a constant symbol, $x$ is a variable, and $X$ is a
relational variable.

\emph{Atomic formulas} are well-formed formulas that do not have any
logical operators, i.e. either of the form
\begin{enumerate}
  \item $\bot$, or
  \item $t_1 = t_2$ where $t_1$ and $t_2$ are terms, or
  \item $R^k(t_1, \dots, t_k)$ where $R^k$ is a $k$-ary relational
    symbol and $t_1, \dots, t_k$ are terms, or
  \item $X^k(t_1, \dots, t_k)$ where $X^k$ is a $k$-ary relational
    variable and $t_1, \dots, t_k$ are terms.
\end{enumerate}

Finally, a \emph{literal} is a formula that is either atomic or the
negation of an atomic formula.

\begin{remark}
  It is interesting to note that in second-order logic we can
  actually define the identity $t_1 = t_2$ as $\forall X (X(t_1)
  \leftrightarrow X(t_2))$ and prove the familiar axioms of identity
  from properties of the implication.
\end{remark}

To define the semantics of second-order logic, we first need to
establish the \emph{metalanguage} we will use. This requirement has
nothing to do with second-order logic but is rather a general feature
of semantics due to Tarski \cite{tarski1956concept}. The most common
choice for a metalanguage is \emph{set theory}. We thus give a
set-theoretical interpretation of second-order logic, where
``properties'' are understood as sets within a domain, which itself
is a set. This set-theoretical interpretation is standard and
highlights the key features of second-order logic.

\begin{remark}
  It's important to note, however, that not all properties can be
  represented as setsâ€”for example, the property of being identical to
  oneself. But when we treat the domain of individual variables as a
  set, we can meaningfully interpret the properties of individuals
  within that domain as sets. If we need to interpret second-order
  logic in a domain too large to be a set, we can instead use the
  set-theoretical concept of a \emph{class}. (For a discussion on the
    set/class distinction in the context of second-order logic, see
  Shapiro \cite{shapiro1991foundations}.)
\end{remark}

Like in the case of first-order logic, the semantics of formulas in
second-order logic are defined with respect to a \emph{structure} (or
equivalently, a \emph{model}). Recall that a structure is a
\emph{universe} along with an \emph{interpretation} of the constant
symbols and relation symbols in the signature. To define whether a
formula holds in a structure, we also need an \emph{assignment},
which interprets free variables. For first-order logic, an assignment
was simply a mapping of variables to elements in the universe of the
structure. For second-order logic, however, the presence of
relational variables, means that an assignment must also give an
interpretation of these variables as relations (of the appropriate
arity) over the structure. These are formally defined as follows.

\begin{definition}[Structures in second-order logic]
  A \emph{$\tau$-structure} $\struct$ is a tuple, $(A,
  \set{c^\struct}_{c \in \tau}, \set{R^\struct}_{R \in \tau})$ where
  \begin{enumerate}
    \item $A$ is a non-empty set called the \emph{universe} (or
      \emph{domain}) of the structure,
    \item for each constant symbol $c \in \tau$, $c^\struct \in U$ is
      its interpretation, and
    \item for each $k$-ary relational symbol $R \in \tau$, $R^\struct
      \subseteq U^k$ is its interpretation.
  \end{enumerate}
\end{definition}

The structure $\struct$ is said to be \emph{finite} if the universe
$A$ is finite. The universe of a structure $\struct$ will be denoted
by $u(\struct)$.

\begin{definition}[Assignments in second-order logic]
  \deflabel{assignments-in-sol}
  Given a $\tau$-structure $\struct$, an \emph{assignment} $\alpha$
  over $\struct$ is a pair of functions $(\alpha_1, \alpha_2)$
  defined as follows.
  \begin{enumerate}
    \item $\alpha_1 : \vars_1 \to u(\struct)$ assigns each individual
      variable $x$ to a variable $\alpha_1(x) \in u(\struct)$.
    \item $\alpha_2 : \vars_2 \to \cup_k (u(A))^k$ assigns each
      $k$-ary relational variable $X^k$ to a relation $\alpha_2(X^k)
      \subseteq (u(A))^k$.
  \end{enumerate}
\end{definition}

\begin{abuse}
  From here on, we will extend $\alpha_1$ to apply to any term, not
  just variables. If the term $t$ is a constant symbol $c$, we take
  $\alpha_1(t)$ to be $c^\struct$. For any variable $x$, we will
  apply $\alpha_1$ as per \defref{assignments-in-sol}.
\end{abuse}

We are now ready to define the semantics of a second-order formula in
a structure under an assignment.

\begin{definition}[Satisfaction of second-order formulas]
  The relation $\struct, \alpha \models \phi$ is inductively defined as follows.
  \begin{enumerate}
    \item $\struct, \alpha \not\models \bot$ for all $\struct$ and $\alpha$.
    \item $\struct, \alpha \models t_1 = t_2$ iff $\alpha_1(t_1) =
      \alpha_1(t_2)$.
    \item $\struct, \alpha \models R(t_1, \dots, t_k)$ iff
      $(\alpha_1(t_1), \dots, \alpha_k(t_k)) \in R^\struct$.
    \item $\struct, \alpha \models X^k(t_1, \dots, t_k)$ iff
      $(\alpha_1(t_1), \dots, \alpha_k(t_k)) \in \alpha_2(X^k)$.
    \item $\struct, \alpha \models (\neg \phi)$ iff $\struct, \alpha
      \not\models \phi$.
    \item $\struct, \alpha \models (\phi_1 \vee \phi_2)$ iff
      $\struct, \alpha \models \phi_1$ or $\struct, \alpha \models \phi_2$.
    \item $\struct, \alpha \models (\exists x \phi)$ iff there exists
      $a \in u(\struct)$ such that $\struct, \alpha[x \mapsto a] \models \phi$.
    \item $\struct, \alpha \models (\exists X^k \phi)$ iff there
      exists $S \in (u(\struct))^k$ such that $\struct, \alpha[X^k
      \mapsto S] \models \phi$.
  \end{enumerate}
\end{definition}

The definition above suggests that the assignment of values to
variables (first-order and relational) only matters for the
\emph{free} variables, i.e. those that are not quantified. We extend
the definition of free and bound occurrences to relational variable as well.

\begin{definition}[Free and bound variables in second-order logic]
  In well-formed formulas $(\exists x \phi_1)$ and $(\exists X^k
  \phi_2)$, $\phi_1$ and $\phi_2$ are said to be in scope of the
  quantifiers $\exists x$ and $\exists X^k$, respectively. Every
  occurrence of $x$ and $X^k$ in $(\exists x \phi)$ and $(\exists X^k
  \phi)$, respectively, are said to be \emph{bound}. Any occurrences
  of variables (first-order and relational) that is not bound are
  said to be \emph{free}.
\end{definition}

When comparing different variable assignments and their effect on a
given formula $\phi$, those variables that do not occur in the
formula or are not free variables in the formula should not affect
the fact that a given structure $\struct$ logically implies the
formula or not. We state this intuitive result in the following.

\begin{lemma}[The relevance lemma for second-order logic]
  For a formula $\phi$ and assignments $\alpha$ and $\beta$ that
  agree on all the free (first-order) variables and free relational
  variables of $\phi$, we have $\struct, \alpha \models \phi$ iff
  $\struct, \beta \models \phi$.
\end{lemma}

\emph{Sentences} are formulas with no free (first-order) variables or
relational variables. Thanks to the relevance lemma, this means that
for sentences, the assignment does not determine its satisfaction.

\begin{proposition}
  For a sentence $\phi$ and any two assignments $\alpha$ and $\beta$,
  we have $\struct, \alpha \models \phi$ iff $\struct, \beta \models
  \phi$. Therefore, we write $\struct \models \phi$ if for any
  assignment $\alpha$, $\struct, \alpha \models \phi$.
\end{proposition}

Satisfiability and validity of second-order formulas are defined in
the same way as for first-order logic. In other words, $\phi$ is
\emph{satisfiable} if there is a structure $\struct$ and assignment
$\alpha$ such that $\struct, \alpha \models \phi$; and $\phi$ is
\emph{valid} if for every structure $\struct$ and assignment
$\alpha$, $\struct, \alpha \models \phi$.

\subsection{Monadic second-order logic}
\emph{Monadic second-order logic (MSO)} is an important fragment of
second-order logic. While first-order logic can be seen as the
fragment that does not allow relational variables, MSO is the
fragment where all relational variables are \emph{monadic}, i.e. of
arity 1. Thus, in MSO one can only quantify over sets, as opposed to
more general relations.

\begin{abuse}
  Since all relational variables are monadic, from now on we will
  drop the superscript that indicates the variables arity.
\end{abuse}

\begin{abuse}
  Sometimes, we may also skip parentheses for convenience. For
  instance, instead of writing $R(x)$, we will just write $R x$.
\end{abuse}

\begin{example}
  Consider the signature of graphs $\tau_G = \set{E}$ which consists
  of a single binary relation which denotes the edge relation in the
  graph. We give the following sentence that expresses the fact that
  the edge relation $E$ encodes a graph that is 3-colorable.
  \begin{align*}
    \exists X_1 \exists X_2 \exists X_3 ( & (\forall x (X_1 x \vee
      X_2 x \vee X_3 x)) \wedge \\
      & (\forall x \forall y (E x y \rightarrow (\neg (X_1 x \wedge X_1
    y) \wedge \neg (X_2 x \wedge X_2 y) \wedge \neg (X_3 x \wedge X_3 y)))))
  \end{align*}
  The sentence above is monadic.
\end{example}

\section{Monadic second-order logic on words}
In this section, we will study MSO on a restricted class of
structures, specifically, \emph{word structures}. Before introducing
MSO on words more formally, let us become familiar with it at an
intuitive level. We start by fixing an alphabet $\Sigma = \set{a,
b}$. MSO on words has three types of atomic formulas:
\begin{enumerate}
  \item Formulas of the form $Q_a(x)$ or $Q_b(x)$, where $x$ is a
    variable ranging over the positions of the word. The intended
    meaning of $Q_a(x)$ if ``the letter at position $x$ is $a$,'' and
    the meaning of $Q_b(x)$ is analogous. For instance, the predicate
    ``all letters of the words are $a$s'' is expressed by the formula
    $\forall x \ Q_a(x)$. The language of all words satisfying the
    formula, called just the language of the formula is $\lang(a^\ast)$.
  \item Formulas of the form $x < y$, where $x$ and $y$ range over
    the positions of the word. The intended meaning is ``position $x$
    is smaller than (i.e. lies to the left of) position y.'' For
    example, the predicate ``if some letter is in $a$, then all
    subsequent letters are also in $a$s'' is expressed by the formula
    \[ \forall x \forall y ((Q_a(x) \wedge x < y) \rightarrow Q_a(y)). \]
    The language of the formula is $\lang(b^\ast a^\ast)$. Notice,
    however, that this is so because $\Sigma = \set{a, b}$. If
    $\Sigma = \set{a, b, c}$, then the language of the formula is
    $\lang((b + c)^\ast a^\ast)$.
  \item Formulas of the form $S(x, y)$ where $x$ and $y$ range over
    the positions of the word. The intended meaning is ``$y$ is the
    next position after $x$'' or ``position $x$ plus one is position
    $y$.'' For example, the predicate ``the next letter after $a$ is
    $b$, and the next letter after $b$ is $a$'' is expressed by the formula
    \[ \forall x \forall y (S(x, y) \rightarrow ((Q_a(x) \wedge
    Q_b(y)) \vee (Q_b(x) \wedge Q_a(y)))). \]
    The language of the formula is $\lang((ab)^\ast + (ba)^\ast)$.
\end{enumerate}

\subsection{Syntax and semantics}
Let's start by defining the signature of MSO on words.

\begin{definition}[The signature of $\MSO(\Sigma)$]
  Given an alphabet $\Sigma$, the signature of $\MSO(\Sigma)$ is
  fixed to be $\tau_W = (\set{Q_a}_{a \in \Sigma}, <, S)$ where $Q_a$
  is a unary relation symbol for every $a$ in the alphabet $\Sigma$,
  and $<$, $S$ are binary relation symbols.
\end{definition}

Now, we formally define the syntax of MSO on words, using BNF grammar.

\begin{definition}[The syntax of $\MSO(\Sigma)$]
  The set $\MSO(\Sigma)$ of monadic second-order formulas over an
  alphabet $\Sigma$ is the set of expressions generated by the
  following grammar,
  \[ \phi ::= Q_a(x) \mid x < y \mid S(x, y) \mid (\neg \phi) \mid
  (\phi \vee \phi) \mid (\exists x \phi) \mid (\exists X \phi) \]
  where $a$ is in the alphabet $\Sigma$, $x, y$ are variables, and
  $X$ is a relational variable.
\end{definition}

Finally, a \emph{word structure} is defined as follows. Its universe
consists of the positions in the word, and the relations $Q_a$, $<$,
and $S$ are interpreted in the ``standard'' way.

\begin{definition}[Word structures]
  Given an alphabet $\Sigma$ and a word $w \in \Sigma^\ast$, let $n$
  be the length of $w$. The structure of $w$ is defined by
  \begin{align*}
    \wstruct = ( & \set{1, 2, \dots, n}, \\
      & < = \set{(1, 2), (1, 3), \dots, (1, n), (2, 3), \dots, (2,
      n), \dots, (n - 1, n)}, \\
      & S = \set{(1, 2), (2, 3), (3, 4), \dots, (n - 1, n)}, \\
      & \set{Q_a = \set{x \mid \text{the letter at position $x$ is
    $a$}}}_{a \in \Sigma}).
  \end{align*}
\end{definition}

\todo{Write some examples to round out this section.}

\section{The BÃ¼chi-Elgot-Trakhtenbrot theorem}
The main result we will establish in this section is that the set of
words definable in MSO exactly corresponds to the set of regular languages.

\begin{abuse}
  Note that there is a one-to-one correspondence between elements of
  $\Sigma^\ast$ and word structures over $\Sigma$. Thus, from now on,
  we will use $w$ interchangeably to refer both to an element of
  $\Sigma^\ast$ and to the corresponding word structure.
\end{abuse}

\begin{definition}[MSO-definable languages]
  A language $A$ (or, equivalently, a set of words or word
  structures) over an alphabet $\Sigma$ is said to be
  \emph{definable} in MSO if there exists an MSO sentence $\phi$ over
  the signature $\tau_W = (\set{Q_a}_{a \in \Sigma}, <, S)$ such that
  $A = \set{w \mid w \models \phi}$.
\end{definition}

\begin{theorem}[The BÃ¼chi-Elgot-Trakhtenbrot theorem]
  A language $A$ is definable in MSO if and only if $A$ is regular.
\end{theorem}

There are two directions to establishing this result, and we will
consider them in order.

\begin{remark}
  The BÃ¼chi-Elgot-Trakhtenbrot theorem is of fundamental importance
  in automata theory because it provides a logical characterization
  of the regular languages. BÃ¼chi \cite{buchi1960weak}, Elgot
  \cite{elgot1961decision}, and Trakhtenbrot
  \cite{trakhtenbrot1962finite} each independently proved the theorem.
\end{remark}

\subsection{Regular languages are expressible in $\MSO(\Sigma)$}
In this section, we show that monadic second-order logic on words can
express all regular languages. In other words, given any regular
language $A$, we can construct an MSO sentence $\phi_A$ that defines
the set $A$.

\begin{lemma}[Regular languages are definable in MSO]
  If $A \subseteq \Sigma^\ast$ is regular, then $A$ is definable in
  $\MSO(\Sigma)$.
\end{lemma}

The following presentation of the proof is based on Esparza and
Blondin \cite{esparza2023automata}, with some modifications.

\begin{proof}
  We present a generic procedure that, given a regular language over
  $\Sigma$, represented by a DFA $A$, constructs a formula $\phi_A$
  of $\MSO(\Sigma)$ such that $\lang(\phi_A) = \lang(A)$. Imagine we
  are given an $A$. There is an obvious way to express the membership
  predicate of $\lang(A)$: ``a word belongs to $\lang(A)$ iff the
  last state of its run on $A$ is accepting.'' For this, we introduce
  the \emph{visit record} of a word. The visit record of a word $w$
  is a mapping that assigns to each state $q$, the set of positions
  of $w$ after which the run reaches $q$. It is the inverse of the
  mapping that assigns to each letter of $w$ the state reached by $A$
  after reading it. Formally:

  \begin{definition}
    Let $A = (Q, \Sigma, \delta, q_0, F)$ be a DFA, and let $w = a_1
    \dots a_m$ be a non-empty word over $\Sigma$. The visit record of
    $w$ is the mapping $R_w : Q \to 2^{\set{1, \dots, m}}$ that
    assigns each state $q \in Q$ to the set of positions defined as follows:
    \[ R_w(q) = \set{i \in \set{1, \dots, m} \mid \hat{\delta}(q_0,
    a_1 \dots a_i) = q}. \]
  \end{definition}

  \begin{example}
    \explabel{mso-of-dfa}
    \figref{visit-record-example} shows a DFA, its run on the word $w
    = aabbb$, and the visit record $R_w$. Observe that each position
    belongs to the visit record of exactly one state. In other words,
    $R_w(q_0)$, $R_w(q_1)$, and $R_w(q_2)$ form a partition of the
    set of positions $\set{1, 2, \dots, 5}$.
  \end{example}

  \begin{figure}[ht]
    \figlabel{visit-record-example}
    \begin{minipage}{0.3\textwidth}
      \begin{automata}
        \node[state,initial]   (q_0)                {$q_0$};
        \node[state]           (q_1) at (3, 0)      {$q_1$};
        \node[state,accepting] (q_2) at (1.5, -2.5) {$q_2$};

        \path[->] (q_0) edge                node {$a$} (q_1)
        (q_0) edge [bend left=15] node {$b$} (q_2)
        (q_1) edge [loop above]   node {$a$} (q_1)
        (q_1) edge [bend left=15] node {$b$} (q_2)
        (q_2) edge [bend left=15] node {$b$} (q_0)
        (q_2) edge [bend left=15] node {$a$} (q_1);
      \end{automata}
    \end{minipage}
    \hfill
    \begin{minipage}{0.6\textwidth}
      Run: \hspace{1cm} $q_0 \overset{a}{\longrightarrow} q_1
      \overset{a}{\longrightarrow} q_1 \overset{b}{\longrightarrow}
      q_2 \overset{b}{\longrightarrow} q_0 \overset{b}{\longrightarrow} q_2$ \\
      Position: \hspace{1.6cm} 1 \hspace{0.72cm} 2 \hspace{0.72cm} 3
      \hspace{0.72cm} 4 \hspace{0.72cm} 5
      \begin{align*}
        & R_w(q_0) = \set{4} \\
        & R_w(q_1) = \set{1, 2} \\
        & R_w(q_2) = \set{3, 5}
      \end{align*}
    \end{minipage}
    \caption{Example of a run and a visit record.}
  \end{figure}

  For every non-empty word, ``the run of $A$ on the word is
  accepting'' is equivalent to the predicate ``the last position of
  the word belongs to the visit record of an accepting state.'' Let
  us examine this predicate. Assume the states of $A$ are $\set{q_0,
  q_1, \dots, q_n}$, and imagine we are able to define a macro
  expressing the visit recordâ€”that is, a macro $\VisitRecord(X_0,
  \dots, X_n)$ such that an interpretation models the macro if and
  only if it assigns to each $X_0, \dots, X_n$, the visit record
  $R_w(q_0), \dots, R_w(q_n)$, respectively. The predicate is then
  expressed by the sentence
  \[ \psi_A = \exists X_0 \dots \exists X_n \left( \VisitRecord(X_0,
      \dots, X_n) \wedge \forall x (\last(x) \rightarrow \bigvee_{q_i \in
  F} X_i(x)) \right) \]
  and hence, for every non-empty word $w$, we have $w \in
  \lang(\psi_A)$ iff $w \in \lang(A)$.

  The next step is to handle the empty word, which has no visit
  record. Since the only first-order quantifier of $\psi_A$ is
  universal, we have $\epsilon \models \psi_A$ for every DFA $A$,
  regardless of whether $\epsilon \in \lang(A)$ holds or not, and so
  we cannot define $\phi_A = \psi_A$. This is easy to fix by defining
  $\phi_A$ as follows:
  \begin{align*}
    \phi_A =
    \begin{cases}
      \psi_A & \text{if $\epsilon \in \lang(A)$} \\
      \psi_A \wedge \exists x (\neg (x < x)) & \text{otherwise}
    \end{cases}
  \end{align*}
  After this adjustment, we have $\lang(\phi_A) = \lang(A)$.

  To help illustrate, for \expref{mso-of-dfa}, the corresponding formula is:
  \[ \exists X_0 \exists X_1 \exists X_2 \left( \VisitRecord(X_0,
    X_1, X_2) \wedge \forall x (\last(x) \rightarrow X_2(x)) \right)
  \wedge \exists x (\neg (x < x)) \]

  To complete our proof, we need to construct the macro
  $\VisitRecord(X_0, \dots, X_1)$. For this, note that the visit
  record $R_w$ can also be defined inductively: we first define which
  component $R_w(q)$ of the record contains position 1, and then,
  assuming we know which component contains position $i$, we define
  which component contains position $i + 1$.

  \begin{proposition}
    \prplabel{visit-record-conditions}
    Let $A = (Q, \Sigma, \delta, q_0, F)$ be a DFA, and let $w = a_1
    \dots a_m$ be a non-empty word over $\Sigma$. The visit record
    $R_w : Q \to 2^{\set{1, \dots, m}}$ is a unique mapping
    satisfying the following properties for every $q, q' \in Q$ and
    every $1 \leq i < m$:
    \begin{enumerate}
      \item $1 \in R_w(q)$ iff $q = \delta(q_0, a_1)$, and
      \item if $i \in R_w(q)$ then $i + 1 \in R_w(q')$ where $q' =
        \delta(q, a_i)$.
    \end{enumerate}
  \end{proposition}

  Again, to help illustrate, for \expref{mso-of-dfa}, we get
  \begin{gather*}
    \text{$1 \in R_w(q)$ iff $q = \delta(q_0, a) = q_1$} \\
    \text{$2 \in R_w(q)$ iff $q = \delta(q_1, a) = q_1$} \\
    \text{$3 \in R_w(q)$ iff $q = \delta(q_1, b) = q_2$} \\
    \vdots
  \end{gather*}
  Intuitively, conditions (1) and (2) of
  \prpref{visit-record-conditions} state that the initial position
  belongs to the right component of the visit record and that the
  visit record ``respects'' the transition relation of $A$. We give
  macros $\Init(X_0, \dots, X_n)$ and $\Respect(X_0, \dots, X_n)$
  expressing these predicates, where we assume the states of $A$ to
  be $\set{q_0, q_1, \dots, q_n}$.

  Before we proceed, let us define the following auxiliary macro.
  Given $0 \leq i \leq n$, we express that position $x$ belongs to
  $X_j$ iff $i = j$ by
  \[ \InX_{q_i}(x) = \left( X_i(x) \wedge \bigwedge_{i \neq j} \neg
  (X_i(x) \wedge X_j(x)) \right). \]

  For condition (1), we define
  \[ \Init(X_0, \dots, X_n) = \forall x \left( \bigwedge_{a \in
      \Sigma} (\first(x) \wedge Q_a(x)) \rightarrow \InX_{\delta(q_0,
  a)}(x) \right). \]
  The formula expresses that if the letter at position 1 is an $a$,
  the position $1$ belongs to $X_{\delta(q_0, a)}$ and to no other set.

  \begin{example}
    For the DFA of \figref{visit-record-example} with states
    $\set{q_0, q_1, q_2}$, we get
    \begin{align*}
      & \Init(X_0, X_1, X_2) = \\
      & \qquad \forall x ((\first(x) \wedge Q_a(x)) \rightarrow
      \InX_{q_1}(x)) \wedge ((\first(x) \wedge Q_b(x)) \rightarrow
      \InX_{q_2}(x)).
    \end{align*}
  \end{example}

  For condition (2), we define
  \begin{align*}
    & \Respect(X_0, \dots, X_n) \\
    & \qquad = \forall x \forall y \left( S(x, y) \rightarrow
      \bigwedge_{a \in \Sigma, \ i \in \set{0, \dots, n}} (Q_a(x)
    \wedge X_i(x)) \rightarrow \InX_{\delta(q_i, a)}(y) \right).
  \end{align*}
  The formula expresses that if a position $x$ belongs to $X_i$, and
  the letter at this position is an $a$, then its successor position
  $y$ belongs to $X_{\delta(q_i, a)}$, and to no other set.

  \begin{example}
    For the DFA of \figref{visit-record-example}, this yields
    \begin{align*}
      & \Respect(X_0, \dots, X_n) \\
      & \qquad = \forall x \forall y \left( S(x, y) \rightarrow
        \begin{pmatrix}
          \phantom{\wedge} \enskip (Q_a(x) \wedge X_0(x)) \rightarrow
          \InX_{q_1}(y) \\
          \wedge \enskip (Q_b(x) \wedge X_0(x)) \rightarrow \InX_{q_2}(y) \\
          \wedge \enskip (Q_a(x) \wedge X_1(x)) \rightarrow \InX_{q_1}(y) \\
          \wedge \enskip (Q_b(x) \wedge X_1(x)) \rightarrow \InX_{q_2}(y) \\
          \wedge \enskip (Q_a(x) \wedge X_2(x)) \rightarrow \InX_{q_1}(y) \\
          \wedge \enskip (Q_b(x) \wedge X_2(x)) \rightarrow \InX_{q_0}(y)
      \end{pmatrix} \right).
    \end{align*}
  \end{example}

  Finally, we are done by setting
  \[ \VisitRecord(X_0, \dots, X_n) = \Init(X_0, \dots, X_n) \wedge
  \Respect(X_0, \dots, X_n). \]
\end{proof}

\subsection{Languages expressible in $\MSO(\Sigma)$ are regular}
For now, refer to Esparza and Blondin \cite{esparza2023automata} for
a detailed discussion.
\todo{Write the rest of this.}

\printbibliography[nottype=image]

\end{document}
